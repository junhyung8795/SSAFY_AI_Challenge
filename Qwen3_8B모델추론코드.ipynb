{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lY2ef3IwSUbU"
      },
      "source": [
        "# Baseline v3.1 - Unsloth 최적화 + 최종 학습 버전\n",
        "\n",
        "**주요 변경 사항:**\n",
        "- **K-Fold 제거**: 최적 하이퍼파라미터 탐색 단계 이후, 전체 학습 데이터로 단일 학습을 진행.\n",
        "- **전체 데이터 사용**: `train.csv`의 모든 데이터를 학습에 사용합니다 (샘플링 제거).\n",
        "- **Wandb 연동**: 스텝 수에 대해 학습 과정을 연속적으로 모니터링하기 위해 `wandb`를 사용.\n",
        "- **상세 프롬프트 적용**: Few-Shot 예시와 CoT가 적용된 상세 시스템 프롬프트를 사용.\n",
        "- **LoRA 설정 업데이트**: `r=32`, `lora_alpha=32`, `lora_dropout=0.05`, `use_rslora=True` 등 K-Fold 실험에서 사용된 설정 적용.\n",
        "- **학습 설정 업데이트**: `num_train_epochs=5`, `cosine` 스케줄러, `packing=True` 등을 적용.\n",
        "\n",
        "Colab의 GPU 환경(T4 GPU)에서 개발되었습니다.\n",
        "- **런타임 > 런타임 유형 변경 > T4 GPU 혹은 A100과 고용량 RAM사용**으로 설정."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7HBjYvBZSzqr"
      },
      "source": [
        "# 1. 환경 준비\n",
        "\n",
        "Unsloth, Wandb 및 최신 라이브러리 설치 코드"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "unsloth_install"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "# colab & local PC용 라이브러리 설치\n",
        "import os, re\n",
        "if \"COLAB_\" not in \"\".join(os.environ.keys()):\n",
        "    !pip install unsloth\n",
        "else:\n",
        "    # Do this only in Colab notebooks! Otherwise use pip install unsloth\n",
        "    import torch; v = re.match(r\"[0-9\\.]{3,}\", str(torch.__version__)).group(0)\n",
        "    xformers = \"xformers==\" + (\"0.0.32.post2\" if v == \"2.8.0\" else \"0.0.29.post3\")\n",
        "    !pip install --no-deps bitsandbytes accelerate {xformers} peft trl triton cut_cross_entropy unsloth_zoo\n",
        "    !pip install sentencepiece protobuf \"datasets>=3.4.1,<4.0.0\" \"huggingface_hub>=0.34.0\" hf_transfer\n",
        "    !pip install --no-deps unsloth\n",
        "!pip install transformers==4.57.0\n",
        "!pip install --no-deps trl==0.22.2\n",
        "!pip install wandb scikit-learn albumentations # 추가 라이브러리"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P0wuwJLPTJpQ"
      },
      "source": [
        "# 2. 데이터 준비 및 Wandb 로그인\n",
        "\n",
        "구글 드라이브를 마운트하고 대회 데이터를 압축한 후, Wandb에 로그인"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PXxjjvUpTLMA"
      },
      "outputs": [],
      "source": [
        "# 구글드라이브 마운트\n",
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nW4YNdl0Tn70"
      },
      "outputs": [],
      "source": [
        "# 데이터 압축 해제 (Colab 경로 예시, 환경에 맞게 수정)\n",
        "!unzip -q \"/home/team100/251024/data.zip\" -d \"/home/team100/content/\" # 경로 확인 필요"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "wandb_login"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/team100/.venv/lib/python3.10/site-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'repr' attribute with value False was provided to the `Field()` function, which has no effect in the context it was used. 'repr' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n",
            "  warnings.warn(\n",
            "/home/team100/.venv/lib/python3.10/site-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'frozen' attribute with value True was provided to the `Field()` function, which has no effect in the context it was used. 'frozen' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n",
            "  warnings.warn(\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mqkrwldn0818\u001b[0m (\u001b[33mqkrwldn0818-sw-\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import wandb\n",
        "wandb.login() # API 키 입력"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3WO2OCFiTqDv"
      },
      "source": [
        "# 3. 라이브러리, 데이터, 설정 로드"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "1QXMeMgFT5MT"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Device: cuda\n"
          ]
        }
      ],
      "source": [
        "import os, random\n",
        "import pandas as pd\n",
        "from PIL import Image\n",
        "import torch\n",
        "from datasets import Dataset, DatasetDict\n",
        "from sklearn.model_selection import train_test_split # To split data\n",
        "from unsloth import FastVisionModel\n",
        "from trl import SFTTrainer, SFTConfig\n",
        "from unsloth.chat_templates import get_chat_template\n",
        "from unsloth.trainer import UnslothVisionDataCollator\n",
        "from tqdm import tqdm\n",
        "import math\n",
        "\n",
        "# 이미지 로드 시 픽셀 제한 해제\n",
        "Image.MAX_IMAGE_PIXELS = None\n",
        "\n",
        "# 디바이스 GPU 우선 사용 설정\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(\"Device:\", device)\n",
        "\n",
        "# 설정값 정의\n",
        "MODEL_ID = \"unsloth/Qwen3-VL-8B-Instruct-unsloth-bnb-4bit\" # Unsloth의 8B 4bit 양자화 모델\n",
        "MAX_SEQ_LENGTH = 2048\n",
        "MAX_NEW_TOKENS = 8\n",
        "SEED = 42\n",
        "random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed_all(SEED)\n",
        "\n",
        "# 데이터셋 로드 (전체 데이터 사용)\n",
        "train_df = pd.read_csv(\"/home/team100/train.csv\")\n",
        "test_df  = pd.read_csv(\"/home/team100/test.csv\")\n",
        "\n",
        "# train_df = train_df.sample(n=200, random_state=SEED).reset_index(drop=True) # 샘플링 제거"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cLdJb59EULQv"
      },
      "source": [
        "# 4. Unsloth 모델 및 LoRA 어댑터 로딩\n",
        "\n",
        "Unsloth의 `FastVisionModel`을 사용하여 4bit 양자화된 모델과 토크나이저를 로드하고, 업데이트된 LoRA 설정을 적용."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "NbinSBK_Ubgo"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "==((====))==  Unsloth 2025.10.9: Fast Qwen3_Vl patching. Transformers: 4.57.0.\n",
            "   \\\\   /|    NVIDIA A100-SXM4-40GB. Num GPUs = 1. Max memory: 39.494 GB. Platform: Linux.\n",
            "O^O/ \\_/ \\    Torch: 2.8.0+cu128. CUDA: 8.0. CUDA Toolkit: 12.8. Triton: 3.4.0\n",
            "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.32.post2. FA2 = False]\n",
            " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
            "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Loading checkpoint shards: 100%|██████████| 2/2 [00:04<00:00,  2.01s/it]\n",
            "Unsloth: Dropout = 0 is supported for fast patching. You are using dropout = 0.05.\n",
            "Unsloth will patch all other layers, except LoRA matrices, causing a performance hit.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "trainable params: 87,293,952 || all params: 8,854,417,648 || trainable%: 0.9859\n"
          ]
        }
      ],
      "source": [
        "model, tokenizer = FastVisionModel.from_pretrained(\n",
        "    model_name = MODEL_ID,\n",
        "    max_seq_length = MAX_SEQ_LENGTH,\n",
        "    load_in_4bit = True,\n",
        "    dtype = torch.bfloat16, # Added for potential performance improvement\n",
        "    fast_inference = False,\n",
        "    gpu_memory_utilization = 0.8,\n",
        ")\n",
        "\n",
        "# LoRA 어댑터 추가 (업데이트된 설정)\n",
        "model = FastVisionModel.get_peft_model(\n",
        "    model,\n",
        "    finetune_vision_layers     = True,\n",
        "    finetune_language_layers   = True,\n",
        "    finetune_attention_modules = True,\n",
        "    finetune_mlp_modules       = True,\n",
        "\n",
        "    r = 32,           # Increased from 16\n",
        "    lora_alpha = 32,  # Increased from 16\n",
        "    lora_dropout = 0.05,# Added dropout\n",
        "    bias = \"none\",\n",
        "    random_state = SEED,\n",
        "    use_rslora = True,  # Enabled Rank Stabilized LoRA\n",
        "    loftq_config = None,\n",
        "    use_gradient_checkpointing = \"unsloth\",\n",
        "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"], # Explicitly target modules\n",
        ")\n",
        "\n",
        "model.print_trainable_parameters()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "collator_setup"
      },
      "outputs": [],
      "source": [
        "from trl.trainer.sft_trainer import DataCollatorForVisionLanguageModeling\n",
        "from transformers import AutoProcessor\n",
        "\n",
        "processor = AutoProcessor.from_pretrained(MODEL_ID)\n",
        "collator = DataCollatorForVisionLanguageModeling(processor)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IvJAhuBvUnRe"
      },
      "source": [
        "# 5. 프롬프트 템플릿 및 데이터 포맷팅\n",
        "\n",
        "상세화된 시스템 프롬프트를 사용하여 데이터를 모델 형식에 맞게 변환하고, 훈련/검증 데이터로 분할합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "image_preprocessing"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Map (num_proc=12):   0%|          | 0/3887 [00:00<?, ? examples/s]"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Map (num_proc=12): 100%|██████████| 3887/3887 [00:57<00:00, 67.18 examples/s] \n"
          ]
        }
      ],
      "source": [
        "# Pandas DataFrame을 Hugging Face Dataset으로 변환\n",
        "raw_dataset = Dataset.from_pandas(train_df)\n",
        "\n",
        "# Resize to (512, 512) and handle image loading errors\n",
        "def convert_to_rgb(example):\n",
        "    try:\n",
        "        example[\"decoded_image\"] = Image.open(example[\"path\"]).resize((512, 512)).convert(\"RGB\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading image {example['path']}: {e}\")\n",
        "        # Create a white dummy image\n",
        "        example[\"decoded_image\"] = Image.new(\"RGB\", (512, 512), color = 'white')\n",
        "    return example\n",
        "\n",
        "raw_dataset = raw_dataset.map(convert_to_rgb, num_proc=os.cpu_count()) # Use multiple processes for faster mapping"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "data_formatting_final"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Map (num_proc=12):   0%|          | 0/3887 [00:00<?, ? examples/s]"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Map (num_proc=12): 100%|██████████| 3887/3887 [01:07<00:00, 57.25 examples/s] "
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training dataset size: 3498\n",
            "Validation dataset size: 389\n",
            "Final training sample:\n",
            " {'messages': [{'content': [{'text': '\\n당신은 시각적 질문에 답변하는 도우미입니다.\\n각 이미지와 질문을 보고, 이미지 안의 사물이나 표지판, 문구의 의미를 이해한 뒤\\n제시된 보기(a, b, c, d) 중 하나를 정확히 선택하세요.\\n답변은 반드시 \\'a\\', \\'b\\', \\'c\\', \\'d\\' 중 한 글자만 출력해야 하며, 설명을 덧붙이지 마세요.\\n\\n이미지 안의 사물이나 텍스트의 의미를 문맥 속에서 해석하세요.\\n예를 들어, 속도 제한 표지판이 있다면 그 안의 숫자(예: 40km/h)를 읽고\\n이 지역의 제한 속도가 40km/h라는 뜻임을 이해해야 합니다.\\n이 경우 정답은 ‘a’입니다.\\n\\n표지판, 안내문, 메뉴판, 장치 등의 이미지는 그 내용이 전달하는\\n구체적인 의미(예: 제한속도, 금지사항, 물건 판매 등)를 파악하여\\n이에 맞는 보기를 선택하세요.\\n\\n------------------------------------\\nFew-Shot 예시\\n------------------------------------\\n\\n예시 1\\n이미지: 도시 도로에 원형 표지판이 있고 “40”이라고 적혀 있습니다.\\n질문: \"이 도시의 제한 속도는 얼마인가요?\"\\n보기:\\na. 40 km/h\\nb. 60 km/h\\nc. 80 km/h\\nd. 100 km/h\\n정답: a\\n이 표지판은 40km/h의 제한 속도를 의미하므로 ‘a’가 정답입니다.\\n\\n예시 2\\n이미지: 기념비 옆에 돌탑들이 쌓여 있습니다.\\n질문: \"이 사진에서 볼 수 있는 돌탑들의 주요 목적은 무엇인가요?\"\\n보기:\\na. 기념과 소원을 위한 돌탑 쌓기\\nb. 돌을 분류하기 위한 작업\\nc. 예술 작품 전시를 위한 조형물\\nd. 건축 자재로 사용하기 위한 돌 쌓기\\n정답: a\\n이 돌탑들은 기념비 옆에 쌓은 형태이므로 ‘a’가 정답입니다.\\n\\n예시 3\\n이미지: 거리의 자동판매기\\n질문: \"이 이미지에서 볼 수 있는 장치의 주된 용도는 무엇인가요?\"\\n보기:\\na. 물품을 보관하는 무인 보관함\\nb. 자동판매기\\nc. 음식을 조리하는 전자레인지\\nd. 인터넷 검색용 키오스크\\n정답: b\\n판매기 안에 음료나 간식이 진열되어 있는 것을 보아, 음료나 간식을 판매하는 자동판매기이므로 ‘b’가 정답입니다.\\n\\n예시 4\\n이미지: “No Parking”이라고 적힌 도로 표지판\\n질문: \"이 표지판이 전달하는 의미는 무엇인가요?\"\\n보기:\\na. 주차 가능 구역\\nb. 주차 금지 구역\\nc. 보행자 전용 구역\\nd. 일방통행 구역\\n정답: b\\n이 표지판은 주차를 금지한다는 의미이므로 ‘b’가 정답입니다.\\n\\n예시 5\\n이미지: 카페 메뉴판에 “카푸치노 4.5, 아메리카노 4.0”이 적혀 있습니다.\\n질문: \"이 이미지의 텍스트가 나타내는 정보는 무엇인가요?\"\\n보기:\\na. 카페의 영업 시간\\nb. 음료 메뉴와 가격 정보\\nc. 카페 위치 정보\\nd. 할인 쿠폰 안내\\n정답: b\\n음료명과 가격이 표시된 메뉴판이므로 ‘b’가 정답입니다.\\n', 'type': 'text'}], 'role': 'system'}, {'content': [{'text': None, 'type': 'image'}, {'text': '사진 속 아이스크림의 맛은 무엇일 가능성이 가장 높은가요?\\n(a) 초코맛\\n(b) 딸기맛\\n(c) 레몬맛\\n(d) 키위맛\\n\\n정답을 반드시 a, b, c, d 중 하나의 소문자 한 글자로만 출력하세요.', 'type': 'text'}], 'role': 'user'}, {'content': [{'text': 'd', 'type': 'text'}], 'role': 'assistant'}], 'images': [<PIL.PngImagePlugin.PngImageFile image mode=RGB size=512x512 at 0x776CD81BD810>]}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# 상세화된 시스템 프롬프트 (Few-Shot 예시 포함)\n",
        "SYSTEM_INSTRUCT = (\"\"\"\n",
        "<|im_start|>system\n",
        "당신은 논리적인 추론가이자 시각 분석 전문가입니다.\n",
        "\n",
        "각 이미지와 질문에 대해, 다음 3단계 사고 과정을 거쳐 답변을 생성하세요.\n",
        "1.  **관찰:** 이미지에서 질문과 관련된 핵심적인 시각 정보(사물, 텍스트, 상황 등)를 객관적으로 서술합니다.\n",
        "2.  **추론:** 관찰한 내용을 바탕으로, 질문의 의도에 맞춰 어떤 보기가 정답인지 논리적인 과정을 단계별로 설명합니다.\n",
        "3.  **최종 답변:** 모든 추론을 종합하여, `최종 답변: [알파벳]` 형식으로 명확하게 결론을 내립니다.\n",
        "\n",
        "---\n",
        "**Few-Shot 예시**\n",
        "\n",
        "**예시 1:** 원형 표지판에 “40 km/h”가 적혀 있는 도시 도로\n",
        "질문: \"이 도시의 제한 속도는 얼마인가요?\"\n",
        "보기: (a) 40 km/h (b) 60 km/h (c) 80 km/h (d) 100 km/h\n",
        "**관찰:** 이미지 중앙에 원형 교통 표지판이 있으며, 그 안에 '40 km/h'라는 텍스트가 명확하게 보입니다.\n",
        "**추론:** 질문은 이 지역의 '제한 속도'를 묻고 있습니다. 표지판의 '40 km/h'는 '시속 40킬로미터'를 의미하며, 이는 차량의 최대 허용 속도를 나타내는 교통 규제입니다. 따라서 보기 a가 표지판의 내용과 일치합니다.\n",
        "**최종 답변: a**\n",
        "\n",
        "**예시 2:** 기념비 옆에 쌓여 있는 돌탑들\n",
        "질문: \"이 사진에서 볼 수 있는 돌탑들의 주요 목적은 무엇인가요?\"\n",
        "보기: (a) 기념과 소원을 위한 돌탑 쌓기 (b) 돌을 분류하기 위한 작업 (c) 예술 작품 전시를 위한 조형물 (d) 건축 자재로 사용하기 위한 돌 쌓기\n",
        "**관찰:** 여러 크기의 돌들이 사람이 인위적으로 쌓아 올린 탑 형태를 이루고 있습니다. 주변 배경은 자연 또는 기념 장소로 보입니다.\n",
        "**추론:** 한국 문화에서 이러한 돌탑은 주로 소원을 빌거나 무언가를 기념하기 위해 쌓는 민속적인 행위입니다. 다른 보기인 분류, 예술, 건축 자재 등은 일반적인 돌탑의 목적과 거리가 멉니다. 따라서 보기 a가 가장 적절한 설명입니다.\n",
        "**최종 답변: a**\n",
        "\n",
        "**예시 3:** 거리의 자동판매기\n",
        "질문: \"이 이미지에서 볼 수 있는 장치의 주된 용도는 무엇인가요?\"\n",
        "보기: (a) 물품을 보관하는 무인 보관함 (b) 자동 판매기 (c) 음식을 조리하는 전자레인지 (d) 인터넷 검색용 키오스크\n",
        "**관찰:** 이미지에는 유리창 너머로 캔이나 병과 같은 다양한 음료가 진열된 기계가 보입니다. 돈이나 카드를 넣는 투입구와 하단에 물건이 나오는 배출구가 있습니다.\n",
        "**추론:** 이 장치는 사람이 없이 상품을 진열하고 결제 시스템을 통해 자동으로 판매하도록 설계되었습니다. 이는 '자동 판매기'의 정의와 정확히 일치합니다. 다른 보기인 보관함, 전자레인지, 키오스크는 시각적 증거와 맞지 않습니다.\n",
        "**최종 답변: b**\n",
        "\n",
        "**예시 4:** “No Parking”이라고 적힌 도로 표지판\n",
        "질문: \"이 표지판이 전달하는 의미는 무엇인가요?\"\n",
        "보기: (a) 주차 가능 구역 (b) 주차 금지 구역 (c) 보행자 전용 구역 (d) 일방통행 구역\n",
        "**관찰:** 이미지에는 \"No Parking\"이라는 텍스트가 명확하게 적힌 표준 교통 표지판이 있습니다.\n",
        "**추론:** 질문은 표지판의 의미를 묻고 있습니다. \"No Parking\"이라는 문구는 해당 구역에 차량을 주차하는 것을 명백히 금지한다는 뜻입니다. 보기 b '주차 금지 구역'은 이 의미를 직접적으로 설명합니다.\n",
        "**최종 답변:** b\n",
        "\n",
        "**예시 5:** 카페 메뉴판에 “카푸치노 4.5, 아메리카노 4.0”이 적혀 있음\n",
        "질문: \"이 이미지의 텍스트가 나타내는 정보는 무엇인가요?\"\n",
        "보기: (a) 카페의 영업 시간 (b) 음료 메뉴와 가격 정보 (c) 카페 위치 정보 (d) 할인 쿠폰 안내\n",
        "**관찰:** 텍스트는 \"카푸치노\", \"아메리카노\"와 같은 음료 이름과 그 옆에 \"4.5\", \"4.0\"과 같은 숫자를 나열하고 있습니다.\n",
        "**추론:** 음료 이름과 숫자를 함께 나열하는 것은 카페 메뉴판에서 가격을 표시하는 표준적인 형식입니다. 따라서 이 텍스트는 음료 메뉴와 그 가격 정보를 나타냅니다. 보기 b가 이 내용과 정확히 일치합니다.\n",
        "**최종 답변:** b\n",
        "<|im_end|>\n",
        "\"\"\")\n",
        "\n",
        "\n",
        "def build_mc_prompt(question, a, b, c, d):\n",
        "    return (\n",
        "        f\"{question}\\n\"\n",
        "        f\"(a) {a}\\n(b) {b}\\n(c) {c}\\n(d) {d}\\n\\n\"\n",
        "        \"정답을 반드시 a, b, c, d 중 하나의 소문자 한 글자로만 출력하세요.\"\n",
        "    )\n",
        "\n",
        "# 학습 데이터를 모델의 대화 형식(messages)으로 변환하는 함수\n",
        "def make_conversation(example):\n",
        "    user_text = build_mc_prompt(str(example[\"question\"]), str(example[\"a\"]), str(example[\"b\"]), str(example[\"c\"]), str(example[\"d\"]))\n",
        "    gold = str(example[\"answer\"]).strip().lower()\n",
        "\n",
        "    messages = [\n",
        "        {\"role\":\"system\",\"content\":[{\"type\":\"text\",\"text\":SYSTEM_INSTRUCT}]},\n",
        "        {\n",
        "            \"role\":\"user\",\n",
        "            \"content\":[\n",
        "                {\"type\":\"image\"}, # Placeholder for the image\n",
        "                {\"type\":\"text\",\"text\":user_text}  # The text part of the prompt\n",
        "                ]\n",
        "         },\n",
        "          {\n",
        "              \"role\":\"assistant\",\n",
        "              \"content\":[\n",
        "                  {\"type\":\"text\",\"text\":gold}\n",
        "                  ]\n",
        "            }\n",
        "    ]\n",
        "    # The actual image data is kept separate for the processor\n",
        "    # Check if 'decoded_image' exists before trying to access it\n",
        "    image_data = [example[\"decoded_image\"]] if \"decoded_image\" in example else []\n",
        "    return {\"messages\": messages, \"images\": image_data}\n",
        "\n",
        "# Apply conversation formatting\n",
        "dataset = raw_dataset.map(\n",
        "    make_conversation,\n",
        "    remove_columns=[\"path\", \"a\", \"b\", \"c\", \"d\", \"question\", \"id\", \"answer\", \"decoded_image\"],\n",
        "    num_proc=os.cpu_count() # Use multiple processes\n",
        ")\n",
        "\n",
        "# 훈련/검증 데이터 분리 (전체 포맷팅된 데이터 대상)\n",
        "dataset_split = dataset.train_test_split(test_size=0.1, seed=SEED)\n",
        "train_dataset = dataset_split[\"train\"]\n",
        "valid_dataset = dataset_split[\"test\"]\n",
        "\n",
        "print(\"Training dataset size:\", len(train_dataset))\n",
        "print(\"Validation dataset size:\", len(valid_dataset))\n",
        "print(\"Final training sample:\\n\", train_dataset[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z6trj7t9U-hv"
      },
      "source": [
        "# 6. SFTTrainer를 사용한 최종 파인튜닝\n",
        "\n",
        "전체 학습 데이터를 사용하여 모델 최종 파인튜닝을 진행. 학습 과정은 Wandb를 통해 모니터링되고 출력문 중 View project이후 나오는 링크를 통해 확인 가능."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sft_trainer_final"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Effective Batch Size: 16\n",
            "Total Training Steps: 1094\n",
            "Evaluation Steps: 110\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'bos_token_id': None}.\n",
            "The model is already on multiple devices. Skipping the move to device specified in `args`.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Starting final training...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n",
            "   \\\\   /|    Num examples = 3,498 | Num Epochs = 5 | Total steps = 1,095\n",
            "O^O/ \\_/ \\    Batch size per device = 8 | Gradient accumulation steps = 2\n",
            "\\        /    Data Parallel GPUs = 1 | Total batch size (8 x 2 x 1) = 16\n",
            " \"-____-\"     Trainable parameters = 87,293,952 of 8,854,417,648 (0.99% trained)\n"
          ]
        },
        {
          "data": {
            "text/html": [],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.22.2"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/home/team100/wandb/run-20251026_055726-wk0vu4tb</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/qkrwldn0818-sw-/huggingface/runs/wk0vu4tb' target=\"_blank\">final_train_full_data_8B</a></strong> to <a href='https://wandb.ai/qkrwldn0818-sw-/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/qkrwldn0818-sw-/huggingface' target=\"_blank\">https://wandb.ai/qkrwldn0818-sw-/huggingface</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/qkrwldn0818-sw-/huggingface/runs/wk0vu4tb' target=\"_blank\">https://wandb.ai/qkrwldn0818-sw-/huggingface/runs/wk0vu4tb</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Detected [huggingface_hub.inference] in use.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Use W&B Weave for improved LLM call tracing. Install Weave with `pip install weave` then add `import weave` to the top of your script.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: For more information, check out the docs at: https://weave-docs.wandb.ai/\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Unsloth: Will smartly offload gradients to save VRAM!\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='773' max='1095' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [ 773/1095 1:41:58 < 42:35, 0.13 it/s, Epoch 3.53/5]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>110</td>\n",
              "      <td>0.805500</td>\n",
              "      <td>1.610877</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>220</td>\n",
              "      <td>0.804500</td>\n",
              "      <td>1.608895</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>330</td>\n",
              "      <td>0.803900</td>\n",
              "      <td>1.608039</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>440</td>\n",
              "      <td>0.802800</td>\n",
              "      <td>1.607752</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>550</td>\n",
              "      <td>0.800800</td>\n",
              "      <td>1.608645</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>660</td>\n",
              "      <td>0.800700</td>\n",
              "      <td>1.608775</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>770</td>\n",
              "      <td>3.624700</td>\n",
              "      <td>7.094336</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Unsloth: Not an error, but Qwen3VLForConditionalGeneration does not accept `num_items_in_batch`.\n",
            "Using gradient accumulation will be very slightly less accurate.\n",
            "Read more on gradient accumulation issues here: https://unsloth.ai/blog/gradient\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[14], line 55\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[38;5;66;03m# 학습 시작\u001b[39;00m\n\u001b[1;32m     54\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStarting final training...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 55\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFinal training finished.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     58\u001b[0m \u001b[38;5;66;03m# 최종 평가\u001b[39;00m\n",
            "File \u001b[0;32m~/unsloth_compiled_cache/UnslothSFTTrainer.py:53\u001b[0m, in \u001b[0;36mprepare_for_training_mode.<locals>.wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfor_training\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m     52\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mfor_training()\n\u001b[0;32m---> 53\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     54\u001b[0m \u001b[38;5;66;03m# Return inference mode\u001b[39;00m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfor_inference\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
            "File \u001b[0;32m~/.venv/lib/python3.10/site-packages/transformers/trainer.py:2325\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   2323\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[1;32m   2324\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2325\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2326\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2327\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2328\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2329\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2330\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m<string>:328\u001b[0m, in \u001b[0;36m_fast_inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n",
            "File \u001b[0;32m~/unsloth_compiled_cache/UnslothSFTTrainer.py:1009\u001b[0m, in \u001b[0;36m_UnslothSFTTrainer.training_step\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1007\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mtraining_step\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m   1008\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmaybe_activation_offload_context:\n\u001b[0;32m-> 1009\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m<string>:91\u001b[0m, in \u001b[0;36m_unsloth_training_step\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n",
            "File \u001b[0;32m~/.venv/lib/python3.10/site-packages/accelerate/accelerator.py:2740\u001b[0m, in \u001b[0;36mAccelerator.backward\u001b[0;34m(self, loss, **kwargs)\u001b[0m\n\u001b[1;32m   2738\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlomo_backward(loss, learning_rate)\n\u001b[1;32m   2739\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2740\u001b[0m     \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/.venv/lib/python3.10/site-packages/torch/_tensor.py:647\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    637\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    638\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    639\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    640\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    645\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    646\u001b[0m     )\n\u001b[0;32m--> 647\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    648\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    649\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/.venv/lib/python3.10/site-packages/torch/autograd/__init__.py:354\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    349\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    351\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    352\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    353\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 354\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    355\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    356\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    357\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    358\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    359\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_tuple\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    360\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    361\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    362\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/.venv/lib/python3.10/site-packages/torch/autograd/graph.py:829\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    827\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    828\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 829\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    830\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    831\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    832\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    833\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Error in callback <bound method _WandbInit._post_run_cell_hook of <wandb.sdk.wandb_init._WandbInit object at 0x70244c7219f0>> (for post_run_cell), with arguments args (<ExecutionResult object at 70244f1e4700, execution_count=14 error_before_exec=None error_in_exec= info=<ExecutionInfo object at 70244f1e4bb0, raw_cell=\"# 모델을 학습 모드로 활성화\n",
            "FastVisionModel.for_training(mode..\" store_history=True silent=False shell_futures=True cell_id=vscode-notebook-cell://ssh-remote%2Bteam100/home/team100/%E1%84%8E%E1%85%AE%E1%84%85%E1%85%A9%E1%86%AB%E1%84%8F%E1%85%A9%E1%84%83%E1%85%B3output%5B0%5D__softmax%E1%84%80%E1%85%B5%E1%84%87%E1%85%A1%E1%86%AB.ipynb#X22sdnNjb2RlLXJlbW90ZQ%3D%3D> result=None>,),kwargs {}:\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:asyncio: socket.send() raised exception.\n"
          ]
        }
      ],
      "source": [
        "# 모델을 학습 모드로 활성화\n",
        "FastVisionModel.for_training(model)\n",
        "\n",
        "# 배치 크기 및 스텝 계산\n",
        "per_device_train_batch_size = 8 # 메모리 부족 방지를 위해 원래 값 유지\n",
        "gradient_accumulation_steps = 2   # 메모리 부족 방지를 위해 원래 값 유지\n",
        "effective_batch_size = per_device_train_batch_size * gradient_accumulation_steps\n",
        "num_train_epochs = 5\n",
        "total_train_steps = math.ceil((len(train_dataset) * num_train_epochs) / effective_batch_size)\n",
        "eval_steps = math.ceil(len(train_dataset) / effective_batch_size / 2) # Evaluate twice per epoch\n",
        "logging_steps = 10 # Log every 10 steps\n",
        "\n",
        "print(f\"Effective Batch Size: {effective_batch_size}\")\n",
        "print(f\"Total Training Steps: {total_train_steps}\")\n",
        "print(f\"Evaluation Steps: {eval_steps}\")\n",
        "\n",
        "# SFTTrainer 설정 (업데이트된 파라미터)\n",
        "trainer = SFTTrainer(\n",
        "    model=model,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=valid_dataset,\n",
        "    packing=True, # Enable packing\n",
        "    data_collator=collator,\n",
        "    args=SFTConfig(\n",
        "        per_device_train_batch_size=per_device_train_batch_size,\n",
        "        per_device_eval_batch_size=per_device_train_batch_size * 2, # Can increase eval batch size slightly\n",
        "        gradient_accumulation_steps=gradient_accumulation_steps,\n",
        "        warmup_steps=int(total_train_steps * 0.05), # Warmup for 5% of total steps\n",
        "        num_train_epochs=num_train_epochs,\n",
        "        learning_rate=1e-4,\n",
        "        logging_steps=logging_steps,\n",
        "        eval_strategy=\"steps\",\n",
        "        eval_steps=eval_steps,\n",
        "        optim=\"adamw_8bit\",\n",
        "        weight_decay=0.01,\n",
        "        lr_scheduler_type=\"cosine\", # Use cosine scheduler\n",
        "        seed=SEED,\n",
        "        output_dir=\"outputs_final\",\n",
        "        report_to=\"wandb\", # Report to wandb\n",
        "        run_name=\"final_train_full_data_8B\", # Wandb run name\n",
        "        remove_unused_columns=False,\n",
        "        dataset_kwargs={\"skip_prepare_dataset\": True},\n",
        "        max_length=MAX_SEQ_LENGTH,\n",
        "        save_strategy=\"steps\", # Save checkpoints periodically\n",
        "        save_steps=eval_steps, # Save checkpoint every evaluation step\n",
        "        save_total_limit=2, # Keep only the last 2 checkpoints\n",
        "        load_best_model_at_end=True, # Load the best model based on evaluation loss at the end\n",
        "        metric_for_best_model=\"eval_loss\", # Use eval_loss to determine the best model\n",
        "        greater_is_better=False, # Lower eval_loss is better\n",
        "    ),\n",
        ")\n",
        "\n",
        "# 학습 시작\n",
        "print(\"Starting final training...\")\n",
        "trainer.train()\n",
        "print(\"Final training finished.\")\n",
        "\n",
        "# 최종 평가\n",
        "print(\"Evaluating the best model...\")\n",
        "final_eval_results = trainer.evaluate()\n",
        "print(\"Final Evaluation Results:\", final_eval_results)\n",
        "\n",
        "# Wandb 종료\n",
        "wandb.finish()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oZuYBgypVItX"
      },
      "source": [
        "# 7. 추론 및 제출 파일 생성\n",
        "\n",
        "학습된 최종 모델(가장 좋은 성능을 보인 체크포인트)을 사용하여 테스트 데이터에 대한 추론을 수행하고, `submission.csv` 파일을 생성 후 submission으로 제출."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "87S01vC0dCmc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "==((====))==  Unsloth 2025.10.9: Fast Qwen3_Vl patching. Transformers: 4.57.0.\n",
            "   \\\\   /|    NVIDIA A100-SXM4-40GB. Num GPUs = 1. Max memory: 39.494 GB. Platform: Linux.\n",
            "O^O/ \\_/ \\    Torch: 2.8.0+cu128. CUDA: 8.0. CUDA Toolkit: 12.8. Triton: 3.4.0\n",
            "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.32.post2. FA2 = False]\n",
            " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
            "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:accelerate.utils.modeling: We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).\n",
            "Loading checkpoint shards: 100%|██████████| 2/2 [00:04<00:00,  2.04s/it]\n",
            "100%|██████████| 3887/3887 [22:02<00:00,  2.94it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saved /home/team100/submission_1026.csv\n",
            "          id answer\n",
            "0  test_0001      b\n",
            "1  test_0002      b\n",
            "2  test_0003      b\n",
            "3  test_0004      c\n",
            "4  test_0005      c\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "from PIL import Image\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "\n",
        "from unsloth import FastVisionModel\n",
        "from transformers import LogitsProcessorList\n",
        "\n",
        "# 0) 모델 로드\n",
        "CKPT_DIR = \"/home/team100/outputs_final/checkpoint-440\"\n",
        "model, tokenizer = FastVisionModel.from_pretrained(\n",
        "    model_name=CKPT_DIR,\n",
        "    load_in_4bit=True,\n",
        "    dtype=torch.float16,   # Kaggle T4 → fp16 권장\n",
        "    device_map=\"auto\",\n",
        ")\n",
        "FastVisionModel.for_inference(model)\n",
        "\n",
        "# --- (중요) Processor 내부 텍스트 토크나이저 가져오기 헬퍼 ---\n",
        "def get_text_tokenizer(tkr):\n",
        "    # 우선순위: .tokenizer > .text_tokenizer > 본인\n",
        "    if hasattr(tkr, \"tokenizer\") and tkr.tokenizer is not None:\n",
        "        return tkr.tokenizer\n",
        "    if hasattr(tkr, \"text_tokenizer\") and tkr.text_tokenizer is not None:\n",
        "        return tkr.text_tokenizer\n",
        "    return tkr  # fallback (혹시 순수 토크나이저인 경우)\n",
        "\n",
        "txt_tok = get_text_tokenizer(tokenizer)\n",
        "\n",
        "# --- 한 글자 선택(a/b/c/d)용 단일 토큰 id들을 최대한 안전하게 모으기 ---\n",
        "def gather_single_token_ids(txt_tok, ch: str):\n",
        "    \"\"\"\n",
        "    txt_tok.encode로 ch, ' '+ch, 대소문자 변형을 모두 시도하여\n",
        "    '하나의 토큰'으로 인코딩되는 경우의 id만 모아 반환.\n",
        "    \"\"\"\n",
        "    candidates = [ch, \" \" + ch, ch.lower(), \" \" + ch.lower(), ch.upper(), \" \" + ch.upper()]\n",
        "    ids = set()\n",
        "    for s in candidates:\n",
        "        try:\n",
        "            toks = txt_tok.encode(s, add_special_tokens=False)\n",
        "        except TypeError:\n",
        "            # 일부 토크나이저는 encode 시 키워드 인자 형태만 받기도 함\n",
        "            toks = txt_tok.encode(s)\n",
        "        if isinstance(toks, dict) and \"input_ids\" in toks:\n",
        "            toks = toks[\"input_ids\"]\n",
        "        if isinstance(toks, (list, tuple)) and len(toks) == 1:\n",
        "            ids.add(int(toks[0]))\n",
        "    return list(ids)\n",
        "\n",
        "A_IDS = gather_single_token_ids(txt_tok, \"a\")\n",
        "B_IDS = gather_single_token_ids(txt_tok, \"b\")\n",
        "C_IDS = gather_single_token_ids(txt_tok, \"c\")\n",
        "D_IDS = gather_single_token_ids(txt_tok, \"d\")\n",
        "\n",
        "# (선택) a/b/c/d 외 토큰 금지\n",
        "class RestrictToSet:\n",
        "    def __init__(self, allowed_ids):\n",
        "        self.allowed = torch.tensor(sorted(list(allowed_ids)))\n",
        "    def __call__(self, input_ids, scores):\n",
        "        # scores: [batch, vocab_size]\n",
        "        mask = scores.new_full(scores.shape, float(\"-inf\"))\n",
        "        mask[:, self.allowed] = scores[:, self.allowed]\n",
        "        return mask\n",
        "\n",
        "USE_VOCAB_RESTRICT = True\n",
        "allowed_all = set(A_IDS + B_IDS + C_IDS + D_IDS)\n",
        "logits_processors = LogitsProcessorList([RestrictToSet(allowed_all)]) if (USE_VOCAB_RESTRICT and len(allowed_all) > 0) else None\n",
        "\n",
        "# 경로/상수\n",
        "ROOT = \"/home/team100/\"     # 이미지 루트\n",
        "SAVE_PATH = \"/home/team100/submission_1026.csv\"\n",
        "MAX_NEW_TOKENS = 1\n",
        "\n",
        "# 추론 루프 (test_df, SYSTEM_INSTRUCT, build_mc_prompt 가 이미 정의돼 있다고 가정)\n",
        "preds = []\n",
        "model.eval()\n",
        "with torch.inference_mode():\n",
        "    for i in tqdm(range(len(test_df))):\n",
        "        row = test_df.iloc[i]\n",
        "        img_path = os.path.join(ROOT, str(row[\"path\"]))\n",
        "        image = Image.open(img_path).convert(\"RGB\")\n",
        "\n",
        "        user_text = build_mc_prompt(row[\"question\"], row[\"a\"], row[\"b\"], row[\"c\"], row[\"d\"])\n",
        "\n",
        "        messages = [\n",
        "            {\"role\": \"system\", \"content\": [{\"type\": \"text\", \"text\": SYSTEM_INSTRUCT}]},\n",
        "            {\"role\": \"user\", \"content\": [{\"type\": \"image\"}, {\"type\": \"text\", \"text\": user_text}]},\n",
        "        ]\n",
        "        prompt = tokenizer.apply_chat_template(\n",
        "            messages,\n",
        "            tokenize=False,\n",
        "            add_generation_prompt=True,\n",
        "        )\n",
        "\n",
        "        inputs = tokenizer(\n",
        "            image,\n",
        "            prompt,\n",
        "            add_special_tokens=False,\n",
        "            return_tensors=\"pt\",\n",
        "        ).to(\"cuda\")\n",
        "\n",
        "        gen_kwargs = dict(\n",
        "            max_new_tokens=MAX_NEW_TOKENS,\n",
        "            do_sample=False,\n",
        "            temperature=0.0,\n",
        "            top_p=1.0,\n",
        "            use_cache=True,\n",
        "            return_dict_in_generate=True,\n",
        "            output_scores=True,\n",
        "        )\n",
        "        if logits_processors is not None:\n",
        "            gen_kwargs[\"logits_processor\"] = logits_processors\n",
        "\n",
        "        outputs = model.generate(**inputs, **gen_kwargs)\n",
        "\n",
        "        # 마지막 토큰 확률분포\n",
        "        scores = outputs.scores[-1][0]   # [vocab_size]\n",
        "        probs = F.softmax(scores, dim=-1)\n",
        "\n",
        "        # 각 보기 확률 = 해당 보기의 가능한 단일토큰 id들의 확률 합\n",
        "        def prob_of(ids):\n",
        "            if not ids:\n",
        "                return -1.0  # 안전장치 (토큰이 전혀 못 모였을 경우)\n",
        "            return float(probs[ids].sum().item())\n",
        "\n",
        "        p_a = prob_of(A_IDS)\n",
        "        p_b = prob_of(B_IDS)\n",
        "        p_c = prob_of(C_IDS)\n",
        "        p_d = prob_of(D_IDS)\n",
        "\n",
        "        probs_dict = {\"a\": p_a, \"b\": p_b, \"c\": p_c, \"d\": p_d}\n",
        "        pred = max(probs_dict, key=probs_dict.get)\n",
        "\n",
        "        preds.append({\"id\": row[\"id\"], \"answer\": pred})\n",
        "\n",
        "# 저장\n",
        "submission = pd.DataFrame(preds, columns=[\"id\", \"answer\"])\n",
        "submission.to_csv(SAVE_PATH, index=False)\n",
        "print(f\"Saved {SAVE_PATH}\")\n",
        "print(submission.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FRM2OE5Kb78P"
      },
      "outputs": [],
      "source": [
        "# 나의 구글 드라이브 본래 작업 폴더에 저장\n",
        "# drive_path = \"/content/drive/My Drive/251024/submission_final.csv\" # 경로 확인 필요, 파일명 변경\n",
        "# submission.to_csv(drive_path, index=False)\n",
        "# print(f\"Saved to Google Drive: {drive_path}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tTcXmx3VeBYo"
      },
      "outputs": [],
      "source": [
        "# 최종 학습된 LoRA 어댑터 저장 (옵션)\n",
        "# model.save_pretrained(\"final_model_lora\")\n",
        "# tokenizer.save_pretrained(\"final_model_lora\")\n",
        "# print(\"Saved LoRA adapter and tokenizer to 'final_model_lora'\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv (3.10.12)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
